{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Grishma5278/Info-5731/blob/main/Tallapareddy_grishma_exercise_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following five feature types are helpful for developing a sentiment analysis machine learning model:\n",
        "\n",
        "1.Features of word frequency\n",
        "2. Features for parts of speech\n",
        "3. Features of sentiment lexicon\n",
        "4. Features of an emotional tone\n",
        "5. Features of Syntactic Dependency\n",
        "\n",
        "Features and the potential benefits of these features:\n",
        " 1. Features of word frequency:\n",
        "\n",
        "-Features based on word frequency, such as bigrams and unigrams. A method such as Term Frequency-Inverse Document Frequency (TF-IDF) can be used to examine the frequency at which particular terms and phrases occur in each review.\n",
        "-There can be a strong correlation between certain words or phrases and how we feel. Positive rating phrases that exemplify this type of language include \"excellent,\" \"amazing,\" and \"great.\"\n",
        "\n",
        "2. Features of parts of speech:\n",
        "-Derived traits (like nouns, verbs, and adjectives) from the grammatical roles of specific words in the text.\n",
        "-Sentiment can be deduced from word choice (e.g., using positive adjectives and negative verbs).\n",
        "3. Features of the sentiment lexicon:\n",
        "-Features taken from lexicons or dictionaries that group words based on their emotional meanings.\n",
        "-The average of the ratings assigned to individual terms can be used to determine the overall tone of a review.\n",
        "-Sentiment lexicons are helpful for methodically recording the underlying feelings that different words express.\n",
        "4. Features of an emotional tone:\n",
        "-The ability to identify the emotional tone of a text and determine whether it is good or negative.\n",
        " Two techniques that might be applied in this situation are sentiment intensity analysis and emotion analysis.\n",
        "-Emotional tone analysis can reveal the author's actual emotions in the review.\n",
        "5. Syntactic Dependency features:\n",
        "Syntactic features, such as the presence or absence of syntactic patterns (such as subject-verb-object) in the text, are the fifth feature of syntactic dependency.\n",
        "-Sentence structure and grammar can affect how a sentence feels.\n",
        " Negative emotions, for instance, could be communicated using passive speech.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uvBXQcn_kDN5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample review\n",
        "review = \"The battery life of this laptop is terrible. The screen quality is also poor.\"\n",
        "\n",
        "# Tokenize the review\n",
        "tokens = word_tokenize(review)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Lemmatize the tokens\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = pos_tag(lemmatized_tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAGiSrv8nVV9",
        "outputId": "909d25f1-dd29-4192-dd26-877c2a75d1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['The', 'battery', 'life', 'of', 'this', 'laptop', 'is', 'terrible', '.', 'The', 'screen', 'quality', 'is', 'also', 'poor', '.']\n",
            "Filtered Tokens: ['battery', 'life', 'laptop', 'terrible', '.', 'screen', 'quality', 'also', 'poor', '.']\n",
            "Lemmatized Tokens: ['battery', 'life', 'laptop', 'terrible', '.', 'screen', 'quality', 'also', 'poor', '.']\n",
            "POS Tags: [('battery', 'NN'), ('life', 'NN'), ('laptop', 'JJ'), ('terrible', 'NN'), ('.', '.'), ('screen', 'JJ'), ('quality', 'NN'), ('also', 'RB'), ('poor', 'JJ'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fed3c0-63ed-47ef-a2ff-d097017871a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Features based on Chi-squared scores (descending order):\n",
            "POS 1: 0.75\n",
            "POS 3: 0.75\n",
            "TF-IDF 2: 0.5500000000000002\n",
            "TF-IDF 4: 0.31999999999999995\n",
            "TF-IDF 1: 0.24499999999999994\n",
            "TF-IDF 3: 0.00357142857142857\n",
            "POS 2: 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# Sample TF-IDF features and part-of-speech features\n",
        "tfidf_features = np.array([[0.2, 0.3, 0.0, 0.4], [0.1, 0.0, 0.5, 0.6], [0.7, 0.8, 0.9, 0.0]])\n",
        "pos_features = np.array([[2, 3, 1], [1, 2, 3], [3, 1, 2]])\n",
        "\n",
        "# Combine features for ranking\n",
        "combined_features = np.concatenate((tfidf_features, pos_features), axis=1)\n",
        "\n",
        "# Sample labels for the samples (positive, negative, neutral)\n",
        "labels = np.array([1, 0, 1])\n",
        "\n",
        "# Calculate chi-squared statistics and p-values\n",
        "chi2_stat, p_val = chi2(combined_features, labels)\n",
        "\n",
        "# Create a dictionary to store feature names and their corresponding chi-squared scores\n",
        "feature_scores = {}\n",
        "feature_names = ['TF-IDF 1', 'TF-IDF 2', 'TF-IDF 3', 'TF-IDF 4', 'POS 1', 'POS 2', 'POS 3']  # Sample feature names\n",
        "for i in range(len(feature_names)):\n",
        "    feature_scores[feature_names[i]] = chi2_stat[i]\n",
        "\n",
        "# Sort features by importance (chi-squared scores) in descending order\n",
        "sorted_features = {k: v for k, v in sorted(feature_scores.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "# Display sorted features and their chi-squared scores\n",
        "print(\"Ranked Features based on Chi-squared scores (descending order):\")\n",
        "for feature, score in sorted_features.items():\n",
        "    print(f\"{feature}: {score}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OZhEGwQohHz",
        "outputId": "d154af16-0996-47db-a65f-0be2e7a8a3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Sample text data\n",
        "texts = [\n",
        "    \"This product is excellent! I am really satisfied with it.\",\n",
        "    \"The customer service was terrible. I had a bad experience.\",\n",
        "    \"The delivery was fast and efficient. Highly recommended!\",\n",
        "]\n",
        "\n",
        "# Sample query\n",
        "query = \"I had a great experience with the customer service.\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to calculate BERT embeddings for a given text\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.pooler_output\n",
        "\n",
        "# Get BERT embeddings for the query and each text\n",
        "query_embedding = get_bert_embedding(query)\n",
        "text_embeddings = [get_bert_embedding(text) for text in texts]\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "similarity_scores = [cosine_similarity(query_embedding, text_embedding)[0][0] for text_embedding in text_embeddings]\n",
        "\n",
        "# Create a list of tuples (text, similarity score)\n",
        "text_similarity_tuples = list(zip(texts, similarity_scores))\n",
        "\n",
        "# Sort the text-similarity tuples by similarity score in descending order\n",
        "sorted_text_similarity = sorted(text_similarity_tuples, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display the sorted text similarity\n",
        "print(\"Ranked Texts based on Similarity (descending order):\")\n",
        "for text, similarity_score in sorted_text_similarity:\n",
        "    print(f\"Similarity Score: {similarity_score:.4f}\")\n",
        "    print(f\"Text: {text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPvaJDg7qJNe",
        "outputId": "9932be30-033e-4fe5-b8f9-c6fff30d26ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Texts based on Similarity (descending order):\n",
            "Similarity Score: 0.9497\n",
            "Text: The delivery was fast and efficient. Highly recommended!\n",
            "\n",
            "Similarity Score: 0.9110\n",
            "Text: The customer service was terrible. I had a bad experience.\n",
            "\n",
            "Similarity Score: 0.8886\n",
            "Text: This product is excellent! I am really satisfied with it.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting features from text data involves several key concepts and techniques, such as tokenization, stop-word removal, lemmatization, and part-of-speech tagging. The most beneficial aspect of this process is understanding how these techniques can be used to preprocess and extract meaningful features from raw text data. The challenge lies in deciding which features to use and how to represent them in a format suitable for machine learning models. This exercise is relevant to the field of NLP as it forms the foundation for many text-based tasks, such as sentiment analysis, document classification, and information retrieval.\n",
        "\n"
      ],
      "metadata": {
        "id": "wvwpe6vLqnat"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "name": "Tallapareddy_grishma_exercise_03.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}