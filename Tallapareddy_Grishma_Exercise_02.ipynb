{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Grishma5278/Info-5731/blob/main/Tallapareddy_Grishma_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research Question: What are the spatial and temporal patterns of bird migration in a given region, and how do these patterns correlate with environmental factors such as climate and habitat changes?\n",
        "\n",
        "Data Needed:\n",
        "\n",
        "Bird Migration Data: To understand the migration patterns, you would need data on the number of birds migrating through the region, the species involved, and the timing of their arrival and departure.\n",
        "Environmental Data: This includes temperature, precipitation, vegetation, and other climate variables that can affect bird migration and habitat.\n",
        "Habitat Data: Information on the types of habitats birds use during migration, such as wetlands, forests, or grasslands, and how these habitats change over time.\n",
        "Amount of Data: The amount of data needed would depend on the size of the region you're studying, the number of bird species involved, and the length of the study period. For a relatively small region (e.g., a single state or province) and a moderate number of bird species (e.g., 50-100), a few years of data might be sufficient. However, for larger regions or more species, you might need data spanning multiple decades.\n",
        "\n",
        "Steps for Data Collection and Saving:\n",
        "\n",
        "Obtain bird migration data from existing databases or citizen science projects, such as eBird or the Global Flyway Network.\n",
        "Collect environmental data from weather stations or other sources, and use GIS (Geographic Information Systems) software to analyze spatial and temporal patterns.\n",
        "Use remote sensing data (e.g., satellite images) to track changes in habitat over time.\n",
        "Save all data in a secure and organized manner, using standardized file formats and metadata to ensure compatibility and reproducibility.\n",
        "Analyze the data using statistical methods and visualization tools to identify trends and correlations between bird migration patterns and environmental factors.\n",
        "Interpret the results and draw conclusions about how climate and habitat changes are affecting bird migration in the region."
      ],
      "metadata": {
        "id": "jDKGK9PaYUQ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Set up a directory to save data\n",
        "if not os.path.exists('bird_migration_data'):\n",
        "    os.mkdir('bird_migration_data')\n",
        "\n",
        "# Function to save data\n",
        "def save_data(data, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(data, file)\n",
        "\n",
        "# Function to collect bird migration data\n",
        "def collect_bird_migration_data(num_samples):\n",
        "    url = 'https://api.ebird.org/v2/data/obs/geo/recent'\n",
        "    # Replace 'your_token' with your actual token obtained from eBird\n",
        "    headers = {'X-eBirdApiToken': 'your_token'}\n",
        "    params = {'lat': 38.9072, 'lng': -77.0369, 'maxResults': num_samples}\n",
        "\n",
        "    response = requests.get(url, headers=headers, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Collect 1000 samples of bird migration data\n",
        "num_samples = 1000\n",
        "data = collect_bird_migration_data(num_samples)\n",
        "\n",
        "# Save the data to a file\n",
        "save_data(data, 'bird_migration_data/bird_migration_data.json')\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "outputId": "331d9ab3-3249-4862-c7ef-3d49984c0f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter num of articles: 100\n",
            "***** BEFORE FETCHING *********\n",
            "******* fetched 10 articles *********\n",
            "******* fetched 20 articles *********\n",
            "******* fetched 30 articles *********\n",
            "******* fetched 40 articles *********\n",
            "******* fetched 50 articles *********\n",
            "******* fetched 60 articles *********\n",
            "******* fetched 70 articles *********\n",
            "******* fetched 80 articles *********\n",
            "******* fetched 90 articles *********\n",
            "******* fetched 100 articles *********\n",
            "******* AFTER FETCHING ********\n",
            "Number of records: 100\n",
            "dimensions of articles: (100, 6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Title  \\\n",
              "0   Information retrieval as statistical translation   \n",
              "1  A survey of automatic query expansion in infor...   \n",
              "2  A language modeling approach to information re...   \n",
              "3  A study of smoothing methods for language mode...   \n",
              "4  Integrating and evaluating neural word embeddi...   \n",
              "\n",
              "                          Author  Year   Published  \\\n",
              "0           A Berger, J Lafferty  2017  dl.acm.org   \n",
              "1          C Carpineto, G Romano  2012  dl.acm.org   \n",
              "2             JM Ponte, WB Croft  2017  dl.acm.org   \n",
              "3             C Zhai, J Lafferty  2017  dl.acm.org   \n",
              "4  G Zuccon, B Koopman, P Bruza…    20  dl.acm.org   \n",
              "\n",
              "                                            Abstract  \\\n",
              "0  … There is a large literature on probabilistic...   \n",
              "1  … information retrieval systems is largely cau...   \n",
              "2  … models, we have developed an approach to ret...   \n",
              "3  … to information retrieval are attractive and ...   \n",
              "4  … in information retrieval. Specifically, we f...   \n",
              "\n",
              "                                        Abstract_UrL  \n",
              "0  https://dl.acm.org/doi/abs/10.1145/3130348.313...  \n",
              "1  https://dl.acm.org/doi/abs/10.1145/2071389.207...  \n",
              "2  https://dl.acm.org/doi/pdf/10.1145/3130348.313...  \n",
              "3  https://dl.acm.org/doi/abs/10.1145/3130348.313...  \n",
              "4  https://dl.acm.org/doi/abs/10.1145/2838931.283...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d012301-efea-4436-ada0-280806dd6730\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Published</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Abstract_UrL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Information retrieval as statistical translation</td>\n",
              "      <td>A Berger, J Lafferty</td>\n",
              "      <td>2017</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>… There is a large literature on probabilistic...</td>\n",
              "      <td>https://dl.acm.org/doi/abs/10.1145/3130348.313...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A survey of automatic query expansion in infor...</td>\n",
              "      <td>C Carpineto, G Romano</td>\n",
              "      <td>2012</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>… information retrieval systems is largely cau...</td>\n",
              "      <td>https://dl.acm.org/doi/abs/10.1145/2071389.207...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A language modeling approach to information re...</td>\n",
              "      <td>JM Ponte, WB Croft</td>\n",
              "      <td>2017</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>… models, we have developed an approach to ret...</td>\n",
              "      <td>https://dl.acm.org/doi/pdf/10.1145/3130348.313...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A study of smoothing methods for language mode...</td>\n",
              "      <td>C Zhai, J Lafferty</td>\n",
              "      <td>2017</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>… to information retrieval are attractive and ...</td>\n",
              "      <td>https://dl.acm.org/doi/abs/10.1145/3130348.313...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Integrating and evaluating neural word embeddi...</td>\n",
              "      <td>G Zuccon, B Koopman, P Bruza…</td>\n",
              "      <td>20</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>… in information retrieval. Specifically, we f...</td>\n",
              "      <td>https://dl.acm.org/doi/abs/10.1145/2838931.283...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d012301-efea-4436-ada0-280806dd6730')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1d012301-efea-4436-ada0-280806dd6730 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1d012301-efea-4436-ada0-280806dd6730');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-29a143e9-29f8-42da-941f-2e3aa4e46233\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-29a143e9-29f8-42da-941f-2e3aa4e46233')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-29a143e9-29f8-42da-941f-2e3aa4e46233 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"Learning deep structured semantic models for web search using clickthrough data\",\n          \"Ensemble machine learning: methods and applications\",\n          \"FEVER: a large-scale dataset for fact extraction and VERification\"\n        ],\n        \"num_unique_values\": 100,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Author\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"J Wan, D Wang, SCH Hoi, P Wu, J Zhu\\u2026\",\n          \"X Wang, X He, M Wang, F Feng, TS Chua\",\n          \"Y Chen, L Xu, K Liu, D Zeng, J Zhao\"\n        ],\n        \"num_unique_values\": 99,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 544,\n        \"min\": 10,\n        \"max\": 2022,\n        \"samples\": [\n          2017,\n          2015,\n          21\n        ],\n        \"num_unique_values\": 19,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Published\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"samples\": [\n          \"nature.com\",\n          \"academia.edu\",\n          \"proceedings.mlr.press\"\n        ],\n        \"num_unique_values\": 30,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"\\u2026 variety of language and information retrieval applications [1][\\u2026 deep auto-encoder for information retrieval. These deep models \\u2026 achieves a superior document retrieval performance to the \\u2026\",\n          \"\\u2026 Today, ensemble learning has many real-world applications, including object detection and tracking, scene segmentation and analysis, image recognition, information retrieval, \\u2026\",\n          \"\\u2026 In order to evaluate the challenge this dataset presents, we developed a pipeline approach that comprises information retrieval and textual entailment components. We showed that the \\u2026\"\n        ],\n        \"num_unique_values\": 100,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abstract_UrL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"https://dl.acm.org/doi/abs/10.1145/2505515.2505665\",\n          \"https://link.springer.com/content/pdf/10.1007/978-1-4419-9326-7.pdf\",\n          \"https://arxiv.org/abs/1803.05355\"\n        ],\n        \"num_unique_values\": 100,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_soup(url):\n",
        "    \"\"\"\n",
        "        params1: url (contains the url of google scholar page)\n",
        "        return: soup (fetching the url page data and then further converted to html parser)\n",
        "    \"\"\"\n",
        "  # headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
        "    try:\n",
        "        # requesting for data using requests with url and headers for authentication\n",
        "        data = requests.get(url, headers)\n",
        "        # print(f\"Extracted the data with response as {data.status_code}\")\n",
        "        if data.status_code != 200:\n",
        "            raise Exception(\"Failed to fetch data\")\n",
        "    except Exception as ex:\n",
        "        print(f\"Exception occurred as {data.text} with status_code {data.status_code}\")\n",
        "        return None\n",
        "    soup = BeautifulSoup(data.content)\n",
        "    return soup\n",
        "\n",
        "def get_title(title):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched title of article\n",
        "      return: string format of text title of article\n",
        "    \"\"\"\n",
        "    return str(title.find(\"a\").text)\n",
        "\n",
        "def get_abstract_url(title):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched title of article\n",
        "      return: string format of url for title of article\n",
        "    \"\"\"\n",
        "    return str(title.find(\"a\").get(\"href\"))\n",
        "\n",
        "\n",
        "def get_article_info(article):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched article info\n",
        "      return: tuples containing author, year, and published info\n",
        "    \"\"\"\n",
        "    # using regular expressions, fetched year from article\n",
        "    year = int(re.search(r'\\d+', article.text).group())\n",
        "    # performing some string operations, fetched required results\n",
        "    article = str(article.text).replace(\"\\xa0\", \"\")\n",
        "    article = article.split(\"-\")\n",
        "    published = article[-1].strip()\n",
        "    author = article[0].strip()\n",
        "    return author, year, published\n",
        "\n",
        "def get_tags(soup):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched url and parsed data\n",
        "      return: list of article info, such as titles, authors, year, published, abstract\n",
        "    \"\"\"\n",
        "    # fetched titles and authors of article using findAll by mentioning some tags\n",
        "    all_titles = soup.findAll(\"h3\", attrs={\"class\": \"gs_rt\"})\n",
        "    all_authors = soup.findAll(\"div\", attrs={\"class\": \"gs_a\"})\n",
        "    all_abstracts = soup.findAll(\"div\", attrs={\"class\": \"gs_rs\"})\n",
        "\n",
        "    authors, year, published = [], [], []\n",
        "\n",
        "    titles = [get_title(title) for title in all_titles]\n",
        "    abs_url = [get_abstract_url(title) for title in all_titles]\n",
        "    abstract = [get_abstract(abstr) for abstr in all_abstracts]\n",
        "\n",
        "    for author in all_authors:\n",
        "        auth, yr, publs = get_article_info(author)\n",
        "        authors.append(auth)\n",
        "        year.append(yr)\n",
        "        published.append(publs)\n",
        "\n",
        "    return titles, authors, year, published, abstract, abs_url\n",
        "\n",
        "\n",
        "def get_abstract(abstr):\n",
        "    \"\"\"\n",
        "      params1: soup_Api with fetched abstract of article\n",
        "      return: string format of article abstract by fetching its text\n",
        "    \"\"\"\n",
        "    return str(abstr.text).replace(\"\\n\", \"\")\n",
        "\n",
        "\n",
        "def fetch_web_data(records):\n",
        "    \"\"\"\n",
        "      params1(records): number of articles, needs to be fetched\n",
        "      return: dataframe containing total N number of articles.\n",
        "    \"\"\"\n",
        "    year_st, year_end = 2012, 2022\n",
        "    columns_google = [\"Title\", \"Author\", \"Year\", \"Published\", \"Abstract\", \"Abstract_UrL\"]\n",
        "    # fetching for 1000 articles\n",
        "    titles, authors, years, published, abstract, abs_url = [], [], [], [], [], []\n",
        "    final_data = []\n",
        "    print(\"***** BEFORE FETCHING *********\")\n",
        "    # records = 100 # no of articles\n",
        "    for i in range(0, records, 10):\n",
        "        url = f\"https://scholar.google.com/scholar?start={i}&q=information+retrieval&hl=en&as_sdt=0,44&as_ylo={year_st}&as_yhi={year_end}&as_vis=1\"\n",
        "        soup = get_soup(url)\n",
        "        if soup is None:\n",
        "          print(f\"Data Not Fetched.... for {i} article page\")\n",
        "          continue\n",
        "        # titles, authors, year, published, abstract, abs_url\n",
        "        a, b, c, d, e, f = get_tags(soup)\n",
        "        titles.extend(a)\n",
        "        authors.extend(b)\n",
        "        years.extend(c)\n",
        "        published.extend(d)\n",
        "        abstract.extend(e)\n",
        "        abs_url.extend(f)\n",
        "        print(f\"******* fetched {(i+10)} articles *********\")\n",
        "        # keeping time to sleep for 5 seconds, so that, server may not crash for frequent multiple requests.\n",
        "        time.sleep(5)\n",
        "\n",
        "    for i in range(records):\n",
        "        final_data.append([titles[i],authors[i],years[i],published[i],abstract[i],abs_url[i]])\n",
        "\n",
        "    print(\"******* AFTER FETCHING ********\")\n",
        "    df = pd.DataFrame(final_data, columns = columns_google)\n",
        "    print(f\"Number of records: {df.shape[0]}\")\n",
        "    return df\n",
        "\n",
        "# url = \"https://scholar.google.com/scholar?start=200&q=information+retrieval&hl=en&as_sdt=0,44&as_ylo=2012&as_yhi=2022&as_vis=1\"\n",
        "# soup = get_soup(url)\n",
        "# all_titles = soup.findAll(\"h3\", attrs={\"class\": \"gs_rt\"})\n",
        "# all_authors = soup.findAll(\"div\", attrs={\"class\": \"gs_a\"})\n",
        "# all_abstracts = soup.findAll(\"div\", attrs={\"class\": \"gs_rs\"})\n",
        "\n",
        "df = fetch_web_data(int(input(\"Enter num of articles: \")))\n",
        "print(f\"dimensions of articles: {df.shape}\")\n",
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install asyncpraw\n",
        "\n",
        "import asyncpraw\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Reddit API using your credentials\n",
        "reddit = asyncpraw.Reddit(client_id='SdBcUkSjP4yOla8lSu8alw',\n",
        "                          client_secret='your_client_secret',\n",
        "                          user_agent='your_user_agent')\n",
        "\n",
        "# Define the subreddit and keywords you want to search for\n",
        "subreddit = await reddit.subreddit('all')\n",
        "keywords = ['Python', 'Data Science', 'Machine Learning']\n",
        "\n",
        "# Define the number of submissions to collect\n",
        "num_submissions = 1000\n",
        "\n",
        "# Collect submissions containing the specified keywords\n",
        "submissions = []\n",
        "async for submission in subreddit.search(keywords, limit=num_submissions):\n",
        "    submissions.append(submission)\n",
        "\n",
        "# Convert submissions to a pandas DataFrame\n",
        "submissions_df = pd.json_normalize([submission.__dict__ for submission in submissions])\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submissions_df.to_csv('reddit_submissions.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "Do98VNgWORUF",
        "outputId": "59c31418-5a3d-452a-8c10-103f808c9ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: aiofiles<1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.8.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.9.3)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.9.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.2.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResponseException",
          "evalue": "received 401 HTTP response",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResponseException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-eada0018e177>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Collect submissions containing the specified keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msubmissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_submissions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0msubmissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/models/listing/generator.py\u001b[0m in \u001b[0;36m__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/models/listing/generator.py\u001b[0m in \u001b[0;36m_next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopAsyncIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_sublist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/util/deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/reddit.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \"\"\"\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objectify_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_deprecate_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fullnames\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"subreddits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/reddit.py\u001b[0m in \u001b[0;36m_objectify_request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[1;32m    566\u001b[0m         return self._objector.objectify(\n\u001b[0;32m--> 567\u001b[0;31m             await self.request(\n\u001b[0m\u001b[1;32m    568\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/util/deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0m_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncpraw/reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mClientException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At most one of 'data' or 'json' is supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             return await self._core.request(\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"api_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         return await self._request_with_retries(\n\u001b[0m\u001b[1;32m    384\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[0;34m(self, data, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mretry_strategy_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         response, saved_exception = await self._make_request(\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, data, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[1;32m    190\u001b[0m     ) -> tuple[ClientResponse, None] | tuple[None, Exception]:\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             response = await self._rate_limiter.call(\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_header_callback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \"\"\"\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/sessions.py\u001b[0m in \u001b[0;36m_set_header_callback\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_set_header_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"refresh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Authorization\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"bearer {self._authorizer.access_token}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/auth.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scopes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0madditional_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scope\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scopes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrant_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"client_credentials\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0madditional_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/auth.py\u001b[0m in \u001b[0;36m_request_token\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requestor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreddit_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACCESS_TOKEN_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mpre_request_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_authenticator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Why are these OKAY responses?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/asyncprawcore/auth.py\u001b[0m in \u001b[0;36m_post\u001b[0;34m(self, url, success_status, **data)\u001b[0m\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msuccess_status\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mResponseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResponseException\u001b[0m: received 401 HTTP response"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "My overall learning experience in working on web scraping tasks has been enlightening. I've gained a deeper understanding of the process of extracting data from various online sources, which is a crucial skill for any professional working with data. One of the key concepts that I found most beneficial was the use of libraries like BeautifulSoup or Scrapy in Python, which provide powerful tools for parsing HTML and XML documents and extracting the information we need. These libraries allow us to navigate the structure of web pages and extract specific elements, such as text, links, and images, making the process of web scraping much more efficient and effective.\n",
        "\n",
        "One of the challenges I encountered in collecting data from certain websites was dealing with websites that use JavaScript to dynamically load content. In such cases, the initial HTML response from the server may not contain all the information we need, and we may need to use techniques like headless browsers or reverse engineering to extract the data we need. Another challenge was dealing with websites that use anti-scraping techniques, such as rate limiting or IP blocking. In such cases, we may need to use proxies or VPNs to disguise our IP address and avoid being detected as a scraper. Overall, web scraping is a valuable skill that can be applied in many different contexts, and I look forward to further developing my skills in this area.\n",
        "\n"
      ],
      "metadata": {
        "id": "TPmTlJgievjZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}