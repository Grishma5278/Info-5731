{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Grishma5278/Info-5731/blob/main/Tallapareddy_Grishma_Exercise_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 5**\n",
        "\n",
        "**This exercise aims to provide a comprehensive learning experience in text analysis and machine learning techniques, focusing on both text classification and clustering tasks.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks***.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission, and no requests will be answered. Manage your time accordingly.**\n"
      ],
      "metadata": {
        "id": "TU-pLW33lpcS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## **Question 1 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text classification** as well as the performance evaluation. In addition, you are requried to conduct **10 fold cross validation** (https://scikit-learn.org/stable/modules/cross_validation.html) in the training.\n",
        "\n",
        "\n",
        "\n",
        "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algorithms:**\n",
        "\n",
        "*   MultinominalNB\n",
        "*   SVM\n",
        "*   KNN\n",
        "*   Decision tree\n",
        "*   Random Forest\n",
        "*   XGBoost\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "**Evaluation measurement:**\n",
        "\n",
        "\n",
        "*   Accuracy\n",
        "*   Recall\n",
        "*   Precison\n",
        "*   F-1 score\n"
      ],
      "metadata": {
        "id": "loi8Sh7UE6ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
        "\n",
        "# Load the training and test data\n",
        "train_data = pd.read_csv('stsa-train.txt', sep='\\t', names=['review', 'label'])\n",
        "test_data = pd.read_csv('stsa-test.txt', sep='\\t', names=['review', 'label'])\n",
        "\n",
        "# Replace missing values with an empty string (customize this based on your data)\n",
        "train_data['review'].fillna('', inplace=True)\n",
        "\n",
        "# Split the training data into features (X) and labels (y)\n",
        "X_train = train_data['review']\n",
        "y_train = train_data['label']\n",
        "\n",
        "# Text vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000, sublinear_tf=True)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Algorithms\n",
        "algorithms = {\n",
        "    'MultinomialNB': MultinomialNB(),\n",
        "    'SVM': SVC(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}\n",
        "\n",
        "# Train and evaluate each algorithm\n",
        "for algo_name, algo in algorithms.items():\n",
        "    print(f\"\\n{algo_name}:\")\n",
        "\n",
        "    # Check information about the target variable\n",
        "    print(\"Target Variable Information:\")\n",
        "    print(f\"Number of NaN values in y_train: {y_train.isnull().sum()}\")\n",
        "    print(f\"Unique values in y_train: {y_train.unique()}\")\n",
        "\n",
        "    # Check if there are enough valid documents for training and if the target variable has NaN values\n",
        "    if X_train_tfidf.shape[0] > 0 and not y_train.isnull().any():\n",
        "        # Train the model\n",
        "        algo.fit(X_train_tfidf, y_train)\n",
        "\n",
        "        # Evaluate using 10-fold cross-validation\n",
        "        scores = cross_validate(algo, X_train_tfidf, y_train, cv=10,\n",
        "                                scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
        "\n",
        "        # Print the average scores\n",
        "        print(f\"Average Accuracy: {scores['test_accuracy'].mean():.4f}\")\n",
        "        print(f\"Average Precision: {scores['test_precision'].mean():.4f}\")\n",
        "        print(f\"Average Recall: {scores['test_recall'].mean():.4f}\")\n",
        "        print(f\"Average F1 Score: {scores['test_f1'].mean():.4f}\")\n",
        "    else:\n",
        "        print(\"Not enough valid documents for training or target variable has NaN values.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQPA7r1_G4jd",
        "outputId": "0d008ca2-b090-4951-eb06-890ce293e0ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MultinomialNB:\n",
            "Target Variable Information:\n",
            "Number of NaN values in y_train: 6920\n",
            "Unique values in y_train: [nan]\n",
            "Not enough valid documents for training or target variable has NaN values.\n",
            "\n",
            "SVM:\n",
            "Target Variable Information:\n",
            "Number of NaN values in y_train: 6920\n",
            "Unique values in y_train: [nan]\n",
            "Not enough valid documents for training or target variable has NaN values.\n",
            "\n",
            "KNN:\n",
            "Target Variable Information:\n",
            "Number of NaN values in y_train: 6920\n",
            "Unique values in y_train: [nan]\n",
            "Not enough valid documents for training or target variable has NaN values.\n",
            "\n",
            "Decision Tree:\n",
            "Target Variable Information:\n",
            "Number of NaN values in y_train: 6920\n",
            "Unique values in y_train: [nan]\n",
            "Not enough valid documents for training or target variable has NaN values.\n",
            "\n",
            "Random Forest:\n",
            "Target Variable Information:\n",
            "Number of NaN values in y_train: 6920\n",
            "Unique values in y_train: [nan]\n",
            "Not enough valid documents for training or target variable has NaN values.\n",
            "\n",
            "XGBoost:\n",
            "Target Variable Information:\n",
            "Number of NaN values in y_train: 6920\n",
            "Unique values in y_train: [nan]\n",
            "Not enough valid documents for training or target variable has NaN values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## **Question 2 (20 Points)**\n",
        "\n",
        "The purpose of the question is to practice different machine learning algorithms for **text clustering**.\n",
        "\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "**Apply the listed clustering methods to the dataset:**\n",
        "*   K-means\n",
        "*   DBSCAN\n",
        "*   Hierarchical clustering\n",
        "*   Word2Vec\n",
        "*   BERT\n",
        "\n",
        "You can refer to of the codes from  the follwing link below.\n",
        "https://www.kaggle.com/karthik3890/text-clustering"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "0X8vj0FbkOzh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv(\"Amazon_Unlocked_Mobile.csv\")\n",
        "# Selecting a subset of the data to speed up computation\n",
        "data = data.sample(frac=0.05, random_state=42)\n"
      ],
      "metadata": {
        "id": "Qz349nJCkSu1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# Remove missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Text Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(data['Reviews'])\n",
        "\n",
        "# PCA for dimensionality reduction\n",
        "pca = PCA(n_components=100)\n",
        "X_pca = pca.fit_transform(X.toarray())"
      ],
      "metadata": {
        "id": "DgX53gE3kXLa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with K-means\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOGlM7BXke3n",
        "outputId": "7aa369de-670c-4701-eedb-aa27dc2ae182"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X_pca)"
      ],
      "metadata": {
        "id": "_CbxWJVVkkKJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with Hierarchical clustering (Agglomerative Clustering)\n",
        "hierarchical = AgglomerativeClustering(n_clusters=5)\n",
        "hierarchical_labels = hierarchical.fit_predict(X_pca)"
      ],
      "metadata": {
        "id": "dcgcgGe1koOU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with Word2Vec\n",
        "word2vec_model = Word2Vec(sentences=[review.split() for review in data['Reviews']], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word_vectors = word2vec_model.wv\n",
        "word_vectors_matrix = word_vectors.vectors\n",
        "word2vec_pca = PCA(n_components=2)\n",
        "word2vec_pca_result = word2vec_pca.fit_transform(word_vectors_matrix)\n",
        "word2vec_labels = KMeans(n_clusters=5, random_state=42).fit_predict(word_vectors_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYXN69Qhk8Hj",
        "outputId": "461c60de-337d-4681-c55f-a824a43a05f6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering with BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "52h2QO06lBCB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 512\n",
        "bert_embeddings = []\n",
        "\n",
        "for review in data['Reviews']:\n",
        "    # Tokenize the review\n",
        "    tokenized_review = tokenizer.encode_plus(review, add_special_tokens=True, max_length=max_seq_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "    inputs = tokenized_review['input_ids']\n",
        "    attention_mask = tokenized_review['attention_mask']\n",
        "\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(inputs, attention_mask=attention_mask)\n",
        "\n",
        "    last_hidden_state = bert_outputs[0][:,0,:].numpy()  # Extract the representation of the [CLS] token\n",
        "    bert_embeddings.append(last_hidden_state)\n",
        "\n",
        "# Concatenate BERT embeddings\n",
        "bert_embeddings_concatenated = np.concatenate(bert_embeddings, axis=0)\n",
        "\n",
        "# Perform clustering with KMeans on BERT embeddings\n",
        "bert_labels = KMeans(n_clusters=5, random_state=42).fit_predict(bert_embeddings_concatenated)\n",
        "\n",
        "# Print cluster labels for each method\n",
        "print(\"K-means labels:\", kmeans_labels)\n",
        "print(\"DBSCAN labels:\", dbscan_labels)\n",
        "print(\"Hierarchical clustering labels:\", hierarchical_labels)\n",
        "print(\"Word2Vec labels:\", word2vec_labels)\n",
        "print(\"BERT labels:\", bert_labels)\n"
      ],
      "metadata": {
        "id": "giPDirnflEt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In one paragraph, please compare the results of K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT.**"
      ],
      "metadata": {
        "id": "4zkX2TqHoNvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write your response here:**\n",
        "\n",
        ".Using the Amazon Unlocked Mobile dataset, the outcomes of applying K-means, DBSCAN, Hierarchical clustering, Word2Vec, and BERT are notably different. Reviews were grouped using K-means clustering based on TF-IDF vectorization. As a result, it produced groups of related content. By utilizing density, DBSCAN identified outliers and highlighted dense areas within reviews. These anomalies could be a sign of a range of opinions.\n",
        "\n",
        " The hierarchical linkages between reviews were identified using hierarchical clustering, which was represented by dendrograms. Word2Vec made it easier to comprehend context and word relationships by capturing semantic similarities between words in the evaluations. A cutting-edge transformer model called BERT provided subtle contextual embeddings that effectively captured intricate patterns in the text. The intended aim determines the algorithm to use. K-means is good at classifying clusters that are clearly separated, Word2Vec for variable density, DBSCAN for variable density, and hierarchical for structural insights.Word2Vec for semantic understanding, and BERT for nuanced contextual analysis. This underscores the importance of selecting the\n",
        "most appropriate technique based on the specific characteristics and objectives of the data.\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pIYCj5qyGfSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment.\n",
        "\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment's exercises provide extensive practical experience with machine learning and text analysis problems. Using robust evaluation through 10-fold cross-validation, the text classification exercise covers a wide range of techniques, such as Naive Bayes, SVM, and Decision Trees. Metrics like accuracy, recall, precision, and F1-score give a clear picture of the model's performance. Similar to this, the text clustering exercise investigates a variety of embedding strategies including Word2Vec and BERT in addition to clustering techniques like K-means, DBSCAN, and hierarchical clustering. Because of their comprehensive explanations and code comments, the well-structured exercises are suitable for a variety of learning levels. They provide insightful information about choosing the right algorithms, preparing text data, and assessing model performance. All things considered, these tasks are great educational tools for professionals that are interested in  NLP and machine learning applications with text data."
      ],
      "metadata": {
        "id": "k7jEuddHgTOI"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}